% ==============================================================================
% APPENDIX A: Data Preprocessing and Dataset Engineering
% File: appendices/appendix_a_preprocessing.tex
% ==============================================================================

\chapter{Appendix A: Data Preprocessing and Dataset Engineering}
\label{app:preprocessing}

This appendix provides a detailed account of the data preprocessing steps applied to the GoEmotions dataset and the methodology used to engineer the three distinct granularities for evaluation. A consistent and well-documented preprocessing pipeline is essential for ensuring the reproducibility and validity of all experimental results.

\section{Source Dataset}

The foundational dataset for this research is the GoEmotions dataset, introduced by Demszky et al. (2020). We utilized the full, original dataset, which contains 58k Reddit comments annotated by 82 raters. The original dataset includes 27 emotion categories plus a 'Neutral' category.

\section{Text Preprocessing Pipeline}

To prepare the text for the classification engines, particularly the lexical (BM25) and statistical (TF-IDF) models which are sensitive to superficial text variations, a standardized preprocessing pipeline was applied to all input texts. This pipeline was designed to normalize the text while preserving as much semantic content as possible for the language model-based engines.

The pipeline consists of the following sequential steps:
\begin{enumerate}
    \item \textbf{Lowercasing:} All text was converted to lowercase. This is a fundamental step to prevent the models from treating the same word with different capitalization as distinct tokens (e.g., treating "Happy" and "happy" as different words).

    \item \textbf{Punctuation Removal:} All standard punctuation marks (e.g., \texttt{.} \texttt{,} \texttt{!} \texttt{?} \texttt{;} \texttt{:} \texttt{'} \texttt{"}) were removed from the text. While punctuation can sometimes carry emotional weight (e.g., an exclamation mark), its inconsistent usage makes it a noisy feature for the classic models. The more advanced LLM, however, was exposed to the original, unprocessed text in later iterations to leverage such signals.

    \item \textbf{Special Character and Number Removal:} All numerical digits and special characters (e.g., \texttt{\$} \texttt{\%} \texttt{\^} \texttt{\&} \texttt{*} \texttt{\~}) were removed. These are generally not indicative of emotion and can add unnecessary complexity to the model's vocabulary.

    \item \textbf{Whitespace Normalization:} All multiple whitespace characters (including tabs and newlines) were collapsed into a single space. Leading and trailing whitespaces were also removed. This ensures consistency in tokenization.
\end{enumerate}

For example, the raw text:
\begin{verbatim}
"WOW, I can't believe I won the lottery!! I'm SO happy!!! :)"
\end{verbatim}
Would be transformed into the following cleaned text after preprocessing:
\begin{verbatim}
"wow i cant believe i won the lottery im so happy"
\end{verbatim}

\section{Granularity Engineering}

A core component of this research was the evaluation across multiple levels of emotional granularity. The three-tiered structure was engineered from the original 27 emotion labels as follows:

\begin{enumerate}
    \item \textbf{27-Label (Fine-Grained):} This is the original, unmodified set of 27 emotion categories from the GoEmotions dataset. The 'Neutral' category was excluded from our analysis to focus specifically on emotional, rather than non-emotional, text. This level represents the most challenging classification task due to the high number of classes and the subtle semantic overlap between them (e.g., 'annoyance' vs. 'anger').

    \item \textbf{15-Label (Intermediate):} This level was created by merging semantically similar or co-occurring emotion labels from the 27-label set. This grouping was informed by the original paper's taxonomy and empirical analysis of label co-occurrence. The goal was to create a mid-level complexity task. Examples of merged categories include:
    \begin{itemize}
        \item \texttt{sadness}, \texttt{grief} $\rightarrow$ \texttt{Sadness/Grief}
        \item \texttt{excitement}, \texttt{joy} $\rightarrow$ \texttt{Excitement/Joy}
        \item \texttt{anger}, \texttt{annoyance}, \texttt{disapproval} $\rightarrow$ \texttt{Anger/Annoyance/Disapproval}
    \end{itemize}

    \item \textbf{3-Label (Coarse):} This level represents the most basic sentiment analysis task. The 27 fine-grained labels were mapped into three broad categories: 'Positive', 'Negative', and 'Ambiguous'.
    \begin{itemize}
        \item \textbf{Positive:} Included emotions such as \texttt{joy}, \texttt{admiration}, \texttt{gratitude}, \texttt{love}, \texttt{optimism}.
        \item \textbf{Negative:} Included emotions such as \texttt{sadness}, \texttt{anger}, \texttt{fear}, \texttt{disgust}, \texttt{disappointment}.
        \item \textbf{Ambiguous:} Included emotions that can have variable valence depending on context, such as \texttt{curiosity}, \texttt{realization}, \texttt{surprise}.
    \end{itemize}
\end{enumerate}

By creating these three parallel versions of the dataset, we established a robust foundation for testing how each classification model's performance changes as the definition of "emotion" moves from highly specific to very general.