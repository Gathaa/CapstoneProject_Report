\chapter{Discussion}
\label{chap:discussion}
\section{Introduction}
The previous chapter presented the empirical results of the proposed hybrid retrieval and N-shot prompting pipeline, structured across three design iterations. While Chapter~\ref{chap:results} focused on factual reporting of metrics and scenarios, the purpose of this chapter is to interpret those findings more deeply. The discussion addresses the meaning of the results in relation to the research questions, situates them within the context of existing literature, and reflects on their practical implications, limitations, and delimitations. Finally, the chapter outlines avenues for future work, providing a broader perspective on the contribution of this research.

\section{Interpretation of Key Findings}
The results demonstrate three core insights. First, performance declined as granularity increased, confirming the inherent difficulty of fine-grained affect recognition. At the 27-label level, the sparsity of certain emotions (e.g., \textit{realization}, \textit{disgust}) reduced the effectiveness of zero-shot prompting. The observed gains from N-shot examples indicate that large language models (LLMs) require contextual guidance to disambiguate subtle emotions, but that their benefit saturates quickly due to model-internal limits on context utilization.  

Second, adaptive N-shot prompting consistently improved performance across all granularities, though optimal $N^{\ast}$ remained small ($N^{\ast}=4$--5). This suggests that LLMs capture generalizable emotional features from only a handful of demonstrations, reflecting findings in prior few-shot learning research \cite{brown2020gpt3}. The plateau effect highlights an important efficiency trade-off: adding more demonstrations increases token cost without proportionate gains in accuracy.  

Third, retrieval-guided prompting demonstrated that the quality of demonstrations matters as much as their quantity. BM25, which relies on keyword overlap, excelled in coarse 3-class sentiment classification where polarity terms dominate, whereas semantic similarity proved stronger in the fine-grained 27-class setting where paraphrases and subtle lexical cues play a role. Hybrid retrieval, combining both signals, consistently yielded the best results across granularities. This outcome confirms that lexical and semantic signals are complementary, and their combination provides robustness across diverse emotional categories.

\section{Research Questions Revisited}
\textbf{RQ1: How can the dataset be prepared and organized to provide a solid foundation for studying emotions in text?}  
The contribution of Iteration~1 was to establish reproducible and aligned datasets across 27, 15, and 3 labels. This answered RQ1 by addressing a critical gap in prior work: inconsistent splits and label mappings that hinder comparability. By ensuring stratified splits and deterministic mappings, this research provided a benchmark that can serve as a stable foundation for replication and extension.

\textbf{RQ2: How does providing different levels of context improve the ability to understand emotions in text?}  
Iteration~2 demonstrated that LLMs are highly sensitive to contextual examples, with performance improving significantly as demonstrations were added. However, the plateau observed at $N^{\ast}=4$--5 shows that LLMs rapidly internalize emotional cues and that excessive prompting yields diminishing returns. This extends prior findings \cite{brown2020gpt3, openai2023gpt4} by quantifying optimal $N$ values at different granularities, offering practical guidance for cost-efficient deployment.

\textbf{RQ3: How can a combined approach that blends different methods lead to more reliable and interpretable results?}  
Iteration~3 confirmed that hybrid retrieval consistently outperformed lexical, semantic, and random baselines. This provides an empirical answer to RQ3: blended retrieval strategies deliver the most reliable and interpretable outcomes. The finding supports earlier retrieval-augmented generation studies \cite{lewis2020retrieval, karpukhin2020dense} while extending them into the domain of emotion recognition.

\section{Comparison with Literature}
These findings align with and extend several strands of existing research. Brown et al. \cite{brown2020gpt3} showed that few-shot prompting boosts performance but quickly saturates. The present study confirms this saturation effect and further demonstrates its variation across granularities. Zhou et al. \cite{zhou2023emotional} argued that LLMs possess a form of emotional intelligence, though often unstable in fine-grained settings. The observed improvements from hybrid retrieval extend their claim by showing that stability can be significantly enhanced with retrieval-guided prompting. Demszky et al. \cite{demszky2020goemotions} highlighted the sparsity challenges of GoEmotions; this study confirms those challenges but mitigates them through adaptive context learning. Finally, retrieval research \cite{lewis2020retrieval, karpukhin2020dense} demonstrated the complementarity of lexical and semantic retrieval in knowledge-intensive tasks. This work extends those insights by proving that the same principle applies to emotionally charged text classification.

\section{Practical Implications}
The findings carry important practical implications. Adaptive N-shot prompting offers a cost-efficient approach for emotion-aware applications such as chatbots, customer support assistants, and moderation tools. By identifying optimal $N^{\ast}$ values, organizations can minimize token usage while maximizing predictive accuracy. Retrieval-guided prompting enhances interpretability by surfacing which examples influenced classification, thereby improving transparency in sensitive applications such as mental health support. Finally, hybrid retrieval ensures robustness across different types of emotions, making the system more suitable for deployment in heterogeneous real-world contexts where inputs range from explicit sentiment to subtle emotional cues.

\section{Limitations}
Despite its contributions, the study is subject to several limitations. The reliance on the GoEmotions dataset introduces domain bias, as the corpus originates from Reddit and may not generalize to other platforms or languages. The exclusive use of GPT-4 deployed via Azure limits the scope of conclusions to a single model-provider pairing, excluding open-source alternatives. Additionally, the interpolation parameter $\lambda$ for hybrid retrieval was fixed at 0.5, preventing exploration of whether different weightings might yield improved performance. Finally, the evaluation focused solely on textual emotion recognition, omitting multimodal signals such as speech and facial expression that play an important role in affective computing.

\section{Delimitations}
Beyond these limitations, deliberate delimitations were imposed to scope the study. First, only English-language text was considered, excluding multilingual or cross-lingual emotion recognition tasks. Second, the evaluation was restricted to single-turn text classification rather than conversational, multi-turn contexts. Third, only three levels of granularity (27, 15, 3 labels) were studied, though alternative taxonomies exist. Finally, the study focused on retrieval-guided prompting rather than fine-tuning, reflecting the aim of developing cost-effective and API-compatible solutions. These delimitations ensured the feasibility of the project within the timeframe of a masterâ€™s thesis while still yielding generalizable insights.

\section{Future Work}
Several extensions are recommended for future research. First, dynamic weighting mechanisms could be introduced to replace the fixed $\lambda=0.5$, allowing the system to learn optimal balances between lexical and semantic retrieval dynamically, perhaps through reinforcement or meta-learning. Second, combining multiple emotion datasets (e.g., EmpatheticDialogues, ISEAR) would mitigate domain bias and broaden coverage of linguistic and cultural contexts. Third, cross-model and cross-platform evaluations, including Amazon Bedrock and open-source LLMs, would test the portability and generality of the pipeline. Finally, extending the approach to multimodal emotion recognition could unlock new applications in human-computer interaction and mental health monitoring.

\section{Summary of Discussion}
In summary, this chapter has interpreted the results of the hybrid retrieval and N-shot prompting pipeline. The discussion demonstrated how each iteration answered its corresponding research question, connected these answers to existing literature, and reflected on their practical significance. While acknowledging limitations and delimitations, the study contributes both empirical evidence and methodological advances to the field of affective computing. By highlighting the importance of both the quantity and quality of contextual information, the research provides a foundation for more adaptive, transparent, and generalizable approaches to emotion recognition in large language models.
